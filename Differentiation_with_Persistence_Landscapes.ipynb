{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "collect-boards",
   "metadata": {},
   "source": [
    "## Distinguishing spheres using persistence landscapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-origin",
   "metadata": {},
   "source": [
    "In this notebook, we will highlight some basic functionality of the `PersLandscapeApprox` class and how it can be used to perform a permutation test through a simple experiment. \n",
    "\n",
    "The experiment is to determine if persistent homology can distinguish between spheres of different dimensions. Here we restrict to differentiating $S^2$ from $S^3$. We will further restrict ourselves to persistent homology in degree one (but one can use all relevant homological degrees as well). A priori, we would not expect this to be an effective discriminator, since the ordinary first homology of both spheres is trivial. Note that this is a simplified version of the experiment from Bubenik and Dlotko's *A persistence landscapes toolbox for topological statistics* [1]. \n",
    "\n",
    "In detail: \n",
    "- Repeat the following `num_runs = 100` times:\n",
    "  - Sample `num_pts = 100` points from the 2-sphere and the 3-sphere. We rescale the spheres so the average distance between the points on each sphere is approximately 1.\n",
    "  - Compute the VR persistent homology (using `ripser`) and compute the associated landscapes. Store each of these landscapes.\n",
    "  - Compute the average landscape for $S^2$ and $S^3$. Take the difference of these two, and finally compute its supremum norm. This yields a real number; call it `significance`.\n",
    "- We have thus established a baseline significance and we will see if the labelling of landscapes by dimension of their underlying sphere is significant as follows. Repeat the following `num_perms = 1000` times:\n",
    "  - Using the stored landscapes, randomly label half as belonging to class A and the other half as class B. \n",
    "  - Compute the average landscape of class A, the average landscape of class B. Compute their difference and its sup norm.\n",
    "  - If the sup norm of a shuffled set of labels is larger than `significance`, then we say that particular shuffling of labels was significant. If the sup norm is smaller than `significance`, then that labelling was not significant.\n",
    "  - Take the total number of significant labellings and divide by `num_perms` to get the p-value of this permutation test. If this p-value is less than some threshold (say 0.05), then we accept the hypothesis that the labelling based on the sphere's dimension is significant. Otherwise, we reject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ripser import ripser\n",
    "from persim.landscapes import (\n",
    "    PersLandscapeApprox, \n",
    "    average_approx, \n",
    "    snap_pl, \n",
    "    plot_landscape, \n",
    "    plot_landscape_simple\n",
    ")\n",
    "from persim import plot_diagrams\n",
    "from tadasets import dsphere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-music",
   "metadata": {},
   "source": [
    "The aforementioned parameters taken from [1]. We modify these parameters below to speed up the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pts = 100\n",
    "num_runs = 100\n",
    "num_perms = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-shaft",
   "metadata": {},
   "source": [
    "#### Using $H_1$ as a discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-albuquerque",
   "metadata": {},
   "source": [
    "Every 1-dimensional loop on $S^2$ and $S^2$ can be continuously shrunk down to a point (for example, by dragging each loop to the 'north pole'). So we would not expect $H_1$ to be a good discriminator between these two hyperspheres. Let's first see what a typical $H_1$ persistence diagram looks like in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-catalog",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sph2 = dsphere(n=num_pts, d=2)/1.3333 # sample points, scaling appropriately\n",
    "sph2_dgm = ripser(sph2, maxdim=2)['dgms']\n",
    "\n",
    "sph3 = dsphere(n=num_pts, d=3)/1.3581 # sample points, scaling appropriately\n",
    "sph3_dgm = ripser(sph3, maxdim=2)['dgms']\n",
    "                                  \n",
    "# Plot persistence diagrams\n",
    "fig, axs = plt.subplots(1, 2, dpi=300)\n",
    "\n",
    "plot_diagrams(sph2_dgm, title=\"Persistence diagram of $S^2$\", ax=axs[0])\n",
    "\n",
    "plot_diagrams(sph3_dgm, title=\"Persistence diagram of $S^3$\", ax=axs[1])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-limit",
   "metadata": {},
   "source": [
    "#### Establish the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-diamond",
   "metadata": {},
   "source": [
    "The following cell samples the points, computes the persistent homology and the landscapes. Depending on the above parameters, it could take a while.\n",
    "\n",
    "We also scale the points according to the average distance between points on $S^2$ and $S^3$, respectively (given here https://math.stackexchange.com/q/2366580/22378). One could compute the actual distance on each run and normalize by that number, but that is more computationally intensive and isn't needed for this simple exercise. Note that `tadasets` implementation of `dsphere` samples points uniformly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "sph2_pl1 = []\n",
    "sph3_pl1 = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    sph2 = dsphere(n=num_pts, d=2)/1.3333 # sample points, scaling appropriately\n",
    "    sph2_dgm = ripser(sph2, maxdim=2)['dgms'] # compute PH0, PH1, PH2\n",
    "    sph2_pl = PersLandscapeApprox(dgms=sph2_dgm, hom_deg=1) # compute persistence landscape\n",
    "    sph2_pl1.append(sph2_pl)\n",
    "    \n",
    "    sph3 = dsphere(n=num_pts, d=3)/1.3581 # sample points, scaling appropriately\n",
    "    sph3_dgm = ripser(sph3, maxdim=2)['dgms'] # compute PH0, PH1, PH2\n",
    "    sph3_pl = PersLandscapeApprox(dgms=sph3_dgm, hom_deg=1) # compute persistence landscape\n",
    "    sph3_pl1.append(sph3_pl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-lotus",
   "metadata": {},
   "source": [
    "We now compute the average landscape of each list of landscapes. This can be done manually, but there is a method `average_approx` that will snap each entry to a common grid and compute the average efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sph2 = average_approx(sph2_pl1)\n",
    "avg_sph3 = average_approx(sph3_pl1)\n",
    "print(avg_sph2, '\\n') \n",
    "print(avg_sph3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-engineering",
   "metadata": {},
   "source": [
    "Each of these average landscapes have been computed, but are on different grids, so we must first snapt them to a common grid before taking their difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "[avg_sph2_snapped, avg_sph3_snapped] = snap_pl([avg_sph2, avg_sph3])\n",
    "true_diff_pl = avg_sph2_snapped - avg_sph3_snapped\n",
    "significance = true_diff_pl.sup_norm()\n",
    "\n",
    "print(f'The threshold for significance is {significance}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-database",
   "metadata": {},
   "source": [
    "Can we see the differences qualititatively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view figures in notebook\n",
    "# %matplotlib inline \n",
    "\n",
    "plot_landscape_simple(avg_sph2_snapped,title='Average $PL_1$ for $S^2$.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_landscape_simple(avg_sph3_snapped,title='Average $PL_1$ for $S^2$.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_landscape_simple(true_diff_pl,title='Difference of average PLs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-crawford",
   "metadata": {},
   "source": [
    "The last figure has a lot going on. We can try the 3d plotting method as well to see more depth. This method is slower, and better used for final figure and not exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_landscape(true_diff_pl,title='Difference of average PLs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-donna",
   "metadata": {},
   "source": [
    "#### Perform the permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_pl = sph2_pl1 + sph3_pl1\n",
    "sig_count = 0\n",
    "\n",
    "for shuffle in range(num_perms):\n",
    "    A_indices = random.sample(range(2*num_runs),num_runs)\n",
    "    B_indices = [_ for _ in range(2*num_runs) if _ not in A_indices]\n",
    "    \n",
    "    A_pl = [comb_pl[i] for i in A_indices]\n",
    "    B_pl = [comb_pl[j] for j in B_indices]\n",
    "    \n",
    "    A_avg = average_approx(A_pl)\n",
    "    B_avg = average_approx(B_pl)\n",
    "    [A_avg_sn, B_avg_sn] = snap_pl([A_avg,B_avg])\n",
    "    \n",
    "    shuff_diff = A_avg_sn - B_avg_sn\n",
    "    if (shuff_diff.sup_norm() >= significance): sig_count += 1\n",
    "\n",
    "pval = sig_count/num_perms\n",
    "\n",
    "print(f'There were {sig_count} shuffles out of {num_perms} that',\n",
    "     'were more significant than the true labelling. Thus, the',\n",
    "     f'p-value is {pval}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-postage",
   "metadata": {},
   "source": [
    "So there wasn't a single run of our experiment that resulted in a more significant labelling than the original one based on each sphere's dimension! Therefore we conclude there is a strong statistical difference between the persistence landscapes in degree one for $S^2$ and $S^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-trigger",
   "metadata": {},
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-essay",
   "metadata": {},
   "source": [
    "[1] Bubenik, P. & Dlotko, P. A persistence landscapes toolbox for topological statistics. Journal of Symbolic Computation **78**, 91–114 (2017).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
