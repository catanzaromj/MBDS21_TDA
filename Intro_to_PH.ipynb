{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facial-thanksgiving",
   "metadata": {},
   "source": [
    "# An elementary introduction to TDA via synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-postage",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce Persistent Homology on several toy data sets. You should try to guess what each data set is sampled from without looking at the scatterplots at the bottom (with the help of Occam's razor). All of these data sets are located in `/data/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-sending",
   "metadata": {},
   "source": [
    "For this notebook, we'll use the `giotto-tda` package. This can be installed via `pip install giotto-tda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-hanging",
   "metadata": {},
   "source": [
    "### Data sets 1 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-phone",
   "metadata": {},
   "source": [
    "Let's load the data and compute its Vietoris-Rips persistent homology in degrees 0, 1, and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.loadtxt('data/synth_data1.txt', delimiter=' ')\n",
    "\n",
    "VR = VietorisRipsPersistence(homology_dimensions=[0,1,2])\n",
    "data1_dgms = VR.fit_transform(data1[None,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagram(data1_dgms[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-nigeria",
   "metadata": {},
   "source": [
    "We find a single homology class in degree 1 (there is always a class in homological degree 0 that lives forever--it is omitted from the persistence diagram). There is no class in homological degree 2, so this eliminates any space with a void, like a standard sphere or a torus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-trader",
   "metadata": {},
   "source": [
    "This data was produced by sampling a nice space exactly with no noise. But real data always has noise! Instead, let's sample from the same space with noise so that you can get a better feel for what persistence diagrams will look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.loadtxt('data/synth_data2.txt', delimiter = ' ')\n",
    "data2_dgms = VR.fit_transform(data2[None,:,:])\n",
    "plot_diagram(data2_dgms[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-claim",
   "metadata": {},
   "source": [
    "Now we see much more 'noise' in the diagram. This refers to the additional $H_1$ classes appearing near the diagonal. There is a slogan in TDA that homology classes lying near the diagonal are 'noise', but this interpretation is highly-dependent on the data set and problem you are trying to solve, so be careful applying this type of logic. It is easy to come up with examples where classes far from the diagonal are noise and those near the diagonal are essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-jones",
   "metadata": {},
   "source": [
    "### Data sets 3 & 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-stanford",
   "metadata": {},
   "source": [
    "Increasing in complexity, let's try to again guess what space we've pulled the following data set of points from just based on its persistent homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = np.loadtxt('data/synth_data3.txt', delimiter=' ')\n",
    "dgms3 = VR.fit_transform(data3[None,:,:])\n",
    "plot_diagram(dgms3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-sodium",
   "metadata": {},
   "source": [
    "This is the first time we've seen 2-dimensional homology away from the diagonal! So this space has one 0-dimensional homology class (the space is connected), one 1-dimensional homology class (one noncontractible loop), and one 2-dimensional homology class (one void). (This one is difficult to guess without having taken a topology course.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-white",
   "metadata": {},
   "source": [
    "Finally, we can look at data set 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = np.loadtxt('data/synth_data4.txt', delimiter=',')\n",
    "data4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-lambda",
   "metadata": {},
   "source": [
    "This data set consists of 400 points in $\\mathbb{R}^4$, so direct visualization of the data set is impossible. One could try using data reduction techniques like PCA to analyze the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "data4_2d = pca2.fit_transform(data4)\n",
    "\n",
    "print(f'2-dimensional PCA has an explained variance ratio of {pca2.explained_variance_ratio_}, for a total of '\n",
    "      f'{pca2.explained_variance_ratio_[0] + pca2.explained_variance_ratio_[1]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-dancing",
   "metadata": {},
   "source": [
    "...but this is not fantastic. Let's see if we can guess using persistent homology again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgms4 = VR.fit_transform(data4[None,:,:])\n",
    "plot_diagram(dgms4[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-wiring",
   "metadata": {},
   "source": [
    "Now we have two 1-dimensional homology classes (close together in the persistence diagram). So `data4` has one connected component, two 1-dimensional homology classes (two distinct loops), and one 2-dimensional homology class (so one void). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-interim",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-integer",
   "metadata": {},
   "source": [
    "Here is the last synthetic data set we'll consider. This persistence diagram reflects/hides periodicity in the data. See if you can find it by zooming in on the diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = np.loadtxt('data/synth_data5.csv',delimiter=',')\n",
    "dgms5 = VR.fit_transform(data5[None,:,:])\n",
    "plot_diagram(dgms5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-grain",
   "metadata": {},
   "source": [
    "In particular, zoom in on the $H_1$ classes which are a bit off the diagonal. How many are there clumped up together? How many $H_1$ classes are far off the diagonal? (This data set is pictured in the slides.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-chrome",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data1[:,0], data1[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data2[:,0],data2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data3[:,0],data3[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data4[:,1],data4[:,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
